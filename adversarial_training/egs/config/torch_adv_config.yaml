dataset:
  train: "weekday"
  eval: "service_detection"
#GPU Congifuration
gpu:
  no_gpu: False
  cuda_ids: "0,1"

classifier:
  learning_rate_classifier: 1e-3 # Learning rate for training classifier
  num_epochs_classifier: 30 # Epoch number for training classifier
  batch_size_train: 512 # Batch size for training classifier
  batch_size_eval: 2048 # Batch size for evaluating classifier
  print_step_classifier: 10 # Step size for showing training progress

# # Certifying classifier
# certify:
#   certify_class: 0 # The certified class (benign class)
#   feature_noise_distribution: "isru" # Feature noise distribution. e.g., "gaussian" "isru" "isru_gaussian" "isru_gaussian_arctan"
#   learning_rate_shape: 1e-2 # Learning rate for optimzing noise shape
#   nt_shape: 1000 # Number of noised samples for optimzing noise shape
#   lambda_shape: 1e-3 # Regularizer weight
#   num_epochs_shape: 5 # Number of epochs for optimzing noise shape
#   d: 100 # Number of feature dimensions
#   num_classes_certify: 2 # Number of certified classes. i.e., Benign(0), Anomaly(1)
#   n0: 100 # Number of noised samples for identify cA
#   n: 10000 # Number of noised samples for estimate pA
#   alpha: 1e-3 # Failure probability
#   init_step_size_scale: 5e-2 # Initial update step size of t for optimzing noise scale
#   init_ptb_t_scale: 1e-2 # Initial perturbation of t for optimzing noise scale
#   decay_factor_scale: 0.5 # Decay factor for optimzing noise scale
#   max_decay_scale: 6 # Maximum decay times for optimzing noise scale
#   max_iter_scale: 100 # Maximum iteration times for optimzing noise scale
#   batch_size_iteration_certify: 128 # Batch size of certified samples for robustness certification
#   batch_size_memory_certify: 1000000 # Batch size of noised samples for robustness certification
#   print_step_certify: 20 # Step size for showing certification progress
